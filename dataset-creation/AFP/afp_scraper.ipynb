{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "f4c9b6a3f12b7421"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T22:08:24.484980Z",
     "start_time": "2025-12-22T22:08:23.578342Z"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "2ef412b49bf82391"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T07:13:25.495858Z",
     "start_time": "2025-12-23T07:13:25.481849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUT_DIR = Path(\"out_afp_verificat\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE = \"https://verificat.afp.com\"\n",
    "SEED_LISTS = [\"https://verificat.afp.com/list/Romania\"]\n",
    "\n",
    "REQUEST_DELAY_SEC = 2.8\n",
    "MAX_RETRIES = 4\n",
    "TIMEOUT = 30\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"ro-RO,ro;q=0.9,en;q=0.7\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Referer\": BASE + \"/\",\n",
    "}\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update(HEADERS)"
   ],
   "id": "f2374e3d9efff80",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils",
   "id": "80beb16bd390d48f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T22:09:11.682789Z",
     "start_time": "2025-12-22T22:09:11.655076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def md5(s: str) -> str:\n",
    "    return hashlib.md5((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def fetch_html(url: str, cache_dir: Path = OUT_DIR / \"cache\") -> str:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cache_path = cache_dir / f\"{sha1(url)}.html\"\n",
    "    if cache_path.exists():\n",
    "        return cache_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    last_err = None\n",
    "    for i in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=TIMEOUT)\n",
    "            if r.status_code == 403:\n",
    "                raise RuntimeError(f\"403 Forbidden for {url}\")\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            cache_path.write_text(html, encoding=\"utf-8\")\n",
    "            time.sleep(REQUEST_DELAY_SEC)\n",
    "            return html\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(REQUEST_DELAY_SEC * (i + 1))\n",
    "    raise RuntimeError(f\"Failed to fetch {url} after {MAX_RETRIES} retries. Last error: {last_err}\")\n",
    "\n",
    "DOC_URL_RE = re.compile(r\"^https?://verificat\\.afp\\.com/doc\\.afp\\.com\\.[A-Za-z0-9]+$\", re.IGNORECASE)\n",
    "\n",
    "def normalize_url(href: str) -> str:\n",
    "    href = (href or \"\").strip()\n",
    "    if not href:\n",
    "        return \"\"\n",
    "    if href.startswith(\"//\"):\n",
    "        href = \"https:\" + href\n",
    "    if href.startswith(\"/\"):\n",
    "        href = BASE + href\n",
    "    return href.split(\"#\")[0]\n",
    "\n",
    "def extract_doc_links(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links: List[str] = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        u = normalize_url(a[\"href\"])\n",
    "        if DOC_URL_RE.match(u):\n",
    "            links.append(u)\n",
    "    return list(dict.fromkeys(links))\n",
    "\n",
    "def find_see_more_url(html: str) -> Optional[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        txt = \" \".join((a.get_text() or \"\").split()).strip().lower()\n",
    "        if \"vezi mai mult\" in txt or \"see more\" in txt:\n",
    "            return normalize_url(a[\"href\"])\n",
    "\n",
    "    a_next = soup.find(\"a\", attrs={\"rel\": lambda x: x and \"next\" in x}, href=True)\n",
    "    if a_next:\n",
    "        return normalize_url(a_next[\"href\"])\n",
    "\n",
    "    return None\n",
    "\n",
    "def discover_urls(seed_url: str, max_steps: int = 80) -> List[str]:\n",
    "    url = seed_url\n",
    "    found: List[str] = []\n",
    "    seen_pages = set()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if not url or url in seen_pages:\n",
    "            break\n",
    "        seen_pages.add(url)\n",
    "\n",
    "        html = fetch_html(url)\n",
    "        found.extend(extract_doc_links(html))\n",
    "\n",
    "        nxt = find_see_more_url(html)\n",
    "        if not nxt:\n",
    "            break\n",
    "        url = nxt\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    return list(dict.fromkeys(found))"
   ],
   "id": "b2ecca75f2b5aa61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## URL Discovery",
   "id": "5cac72ea15b7058c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T07:14:01.137969Z",
     "start_time": "2025-12-23T07:13:31.551557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "discovered = []\n",
    "for seed in SEED_LISTS:\n",
    "    try:\n",
    "        discovered.extend(discover_urls(seed, max_steps=80))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] seed failed: {seed} -> {e}\")\n",
    "\n",
    "discovered = list(dict.fromkeys(discovered))\n",
    "len(discovered), discovered[:10]"
   ],
   "id": "69b94029fbf53e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] seed failed: https://verificat.afp.com/list/Romania -> Failed to fetch https://verificat.afp.com/list/Romania after 4 retries. Last error: 403 Forbidden for https://verificat.afp.com/list/Romania\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12babac91c28e74d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
