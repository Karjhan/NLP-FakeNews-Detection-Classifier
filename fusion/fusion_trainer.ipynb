{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "6f603eb54b1924f4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-27T16:29:13.686836Z",
     "start_time": "2025-12-27T16:29:13.676834Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import json, math, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DATASETS_ROOT = Path(\"../dataset-creation\")\n",
    "\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "OUT_FUSION_DIR = ARTIFACTS_DIR / \"fusion\"\n",
    "OUT_FUSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATHS = {\n",
    "    \"afp\": DATASETS_ROOT / \"AFP\" / \"out_afp_verificat\" / \"afp_verificat_dataset.csv\",\n",
    "    \"factual\": DATASETS_ROOT / \"Factual\" / \"data\" / \"factual_ro_dataset_postprocessed.csv\",\n",
    "    \"veridica\": DATASETS_ROOT / \"Veridica\" / \"data_veridica\" / \"veridica_dataset.csv\",\n",
    "    \"ocr\": DATASETS_ROOT / \"Pseudo-FakeRom\" / \"ocr_fake_news_dataset.csv\",\n",
    "    \"tnr\": DATASETS_ROOT / \"TNR\" / \"out_tnr\" / \"tnr_satire_dataset.csv\",\n",
    "}\n",
    "\n",
    "for k, p in PATHS.items():\n",
    "    print(f\"{k:8s} -> exists={p.exists()}  path={p}\")\n",
    "\n",
    "VERACITY_MODEL_DIR  = Path(\"..\") / \"binary\" / \"models\" / \"veracity_roberta\"\n",
    "CLICKBAIT_MODEL_DIR = Path(\"..\") / \"clickbait\" / \"models\" /\"rocloco_roberta_clickbait\"\n",
    "\n",
    "SOURCE_TABLE_PATH = Path(\"..\") / \"source_veracity\" / \"out_source_prior\" / \"source_veracity_table.csv\"\n",
    "print(\"SOURCE_TABLE exists:\", SOURCE_TABLE_PATH.exists(), SOURCE_TABLE_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afp      -> exists=True  path=..\\dataset-creation\\AFP\\out_afp_verificat\\afp_verificat_dataset.csv\n",
      "factual  -> exists=True  path=..\\dataset-creation\\Factual\\data\\factual_ro_dataset_postprocessed.csv\n",
      "veridica -> exists=True  path=..\\dataset-creation\\Veridica\\data_veridica\\veridica_dataset.csv\n",
      "ocr      -> exists=True  path=..\\dataset-creation\\Pseudo-FakeRom\\ocr_fake_news_dataset.csv\n",
      "tnr      -> exists=True  path=..\\dataset-creation\\TNR\\out_tnr\\tnr_satire_dataset.csv\n",
      "SOURCE_TABLE exists: True ..\\source_veracity\\out_source_prior\\source_veracity_table.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils",
   "id": "4c0f481741833b9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:18:25.405301Z",
     "start_time": "2025-12-27T16:18:25.391784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def safe_str(x) -> str:\n",
    "    import numpy as _np\n",
    "    return \"\" if x is None or (isinstance(x, float) and _np.isnan(x)) else str(x)\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    return normalize_ws(s).upper()\n",
    "\n",
    "def md5(s: str) -> str:\n",
    "    return hashlib.md5((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_domain(u: str) -> str:\n",
    "    u = safe_str(u).strip()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        d = urlparse(u).netloc.lower()\n",
    "        return d.replace(\"www.\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def clamp(p: float, eps: float = 1e-6) -> float:\n",
    "    return float(max(eps, min(1.0 - eps, p)))\n",
    "\n",
    "def logit(p: float) -> float:\n",
    "    p = clamp(p)\n",
    "    return math.log(p / (1 - p))\n",
    "\n",
    "PLATFORM_DOMAINS = {\n",
    "    \"facebook.com\",\"m.facebook.com\",\n",
    "    \"tiktok.com\",\n",
    "    \"youtube.com\",\"youtu.be\",\n",
    "    \"twitter.com\",\"x.com\",\n",
    "    \"instagram.com\",\n",
    "    \"reddit.com\",\n",
    "    \"telegram.org\",\"t.me\",\n",
    "}"
   ],
   "id": "5934ff9927a24c8b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset loaders",
   "id": "8149f422b60b81c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:18:48.997381Z",
     "start_time": "2025-12-27T16:18:48.182509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRUE_SET_RO = {\"ADEVĂRAT\", \"ADEVARAT\", \"PARȚIAL ADEVĂRAT\", \"PARTIAL ADEVARAT\", \"PARTIAL ADEVĂRAT\", \"REAL\", \"TRUE\"}\n",
    "FALSE_SET_RO = {\n",
    "    \"FALS\", \"TRUNCHIAT\", \"ÎNȘELĂTOR\", \"INȘELĂTOR\", \"INSELATOR\", \"CONTEXT LIPSĂ\", \"CONTEXT LIPSA\",\n",
    "    \"LIPSA CONTEXTULUI\", \"FOTOGRAFIE ALTERATĂ\", \"FOTOGRAFIE ALTERATA\",\n",
    "    \"VIDEOCLIP ALTERAT\", \"VIDEO ALTERAT\", \"DEEPFAKE\", \"SATIRĂ\", \"SATIRA\", \"SATIRE\", \"FARSĂ\", \"FARSA\",\n",
    "    \"FAKE\", \"FALSE\", \"FAKE NEWS\", \"DEZINFORMARE\", \"FABRICATED\", \"PROPAGANDA\", \"PROPAGANDĂ\", \"PROPAGANDĂ DE RĂZBOI\"\n",
    "}\n",
    "UNVERIFIABLE_SET_RO = {\"IMPOSIBIL DE VERIFICAT\", \"S-A RĂZGÂNDIT\", \"S-A RAZGANDIT\", \"PLAUSIBLE\"}\n",
    "\n",
    "def map_label_binary(label_fine: str):\n",
    "    L = normalize_label(label_fine)\n",
    "    if L in TRUE_SET_RO:\n",
    "        return 1\n",
    "    if L in FALSE_SET_RO:\n",
    "        return 0\n",
    "    if L in UNVERIFIABLE_SET_RO:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def load_afp(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    out = pd.DataFrame()\n",
    "    out[\"dataset\"] = \"afp\"\n",
    "    out[\"id\"] = df.get(\"id\", pd.Series([None]*len(df))).astype(str)\n",
    "    out[\"url\"] = df.get(\"url\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"source_url\"] = df.get(\"source_url\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"source_domain\"] = out[\"source_url\"].apply(get_domain)\n",
    "    out[\"title\"] = df.get(\"title\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"claim\"] = df.get(\"claim\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"label_fine\"] = df.get(\"label_norm\", df.get(\"label\", \"\")).fillna(\"\").apply(safe_str)\n",
    "    out[\"y\"] = out[\"label_fine\"].apply(map_label_binary)\n",
    "    out[\"text_long\"] = \"\"\n",
    "    out[\"text_short\"] = (out[\"title\"].where(out[\"title\"].str.len()>0, out[\"claim\"]) + \" [SEP] \" + out[\"claim\"]).str.strip()\n",
    "    return out\n",
    "\n",
    "def load_factual(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    out = pd.DataFrame()\n",
    "    out[\"dataset\"] = \"factual\"\n",
    "    out[\"id\"] = df.get(\"id\", pd.Series([None]*len(df))).astype(str)\n",
    "    out[\"url\"] = df.get(\"url\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"source_url\"] = df.get(\"source\", df.get(\"speaker_url\", \"\")).fillna(\"\").apply(safe_str)\n",
    "    out[\"source_domain\"] = out[\"source_url\"].apply(get_domain)\n",
    "    out[\"title\"] = df.get(\"title\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"claim\"] = df.get(\"claim\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"label_fine\"] = df.get(\"label\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"y\"] = out[\"label_fine\"].apply(map_label_binary)\n",
    "    out[\"text_long\"] = df.get(\"text\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"text_short\"] = (out[\"title\"].where(out[\"title\"].str.len()>0, out[\"claim\"]) + \" [SEP] \" + out[\"claim\"]).str.strip()\n",
    "    return out\n",
    "\n",
    "def load_veridica(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    out = pd.DataFrame()\n",
    "    out[\"dataset\"] = \"veridica\"\n",
    "    out[\"id\"] = df.get(\"id\", pd.Series([None]*len(df))).astype(str)\n",
    "    out[\"url\"] = df.get(\"url\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"source_url\"] = \"\"\n",
    "    out[\"source_domain\"] = out[\"url\"].apply(get_domain)\n",
    "    out[\"title\"] = df.get(\"title\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"claim\"] = df.get(\"claim\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"label_fine\"] = df.get(\"label\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"y\"] = 0  # your current assumption for this scrape\n",
    "    out[\"text_long\"] = df.get(\"text\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"text_short\"] = (out[\"title\"].where(out[\"title\"].str.len()>0, out[\"claim\"]) + \" [SEP] \" + out[\"claim\"]).str.strip()\n",
    "    return out\n",
    "\n",
    "def load_ocr(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    out = pd.DataFrame()\n",
    "    out[\"dataset\"] = \"ocr\"\n",
    "    out[\"id\"] = df.get(\"id\", pd.Series([None]*len(df))).astype(str)\n",
    "    out[\"url\"] = \"\"\n",
    "    out[\"source_url\"] = \"\"\n",
    "    out[\"source_domain\"] = \"\"\n",
    "    out[\"title\"] = \"\"\n",
    "    out[\"claim\"] = \"\"\n",
    "    out[\"label_fine\"] = df.get(\"label\", \"\").fillna(\"\").apply(safe_str)\n",
    "    if \"label_group\" in df.columns:\n",
    "        lg = df[\"label_group\"].fillna(\"\").apply(safe_str).str.upper()\n",
    "        out[\"y\"] = lg.map({\"REAL\": 1, \"TRUE\": 1, \"FAKE\": 0, \"FALSE\": 0})\n",
    "    else:\n",
    "        out[\"y\"] = out[\"label_fine\"].apply(map_label_binary)\n",
    "    out[\"text_long\"] = df.get(\"text\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"text_short\"] = \"\"\n",
    "    return out\n",
    "\n",
    "def load_tnr(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    out = pd.DataFrame()\n",
    "    out[\"dataset\"] = \"tnr\"\n",
    "    out[\"id\"] = df.get(\"id\", pd.Series([None]*len(df))).astype(str)\n",
    "    out[\"url\"] = df.get(\"url\", \"\").fillna(\"\").apply(safe_str)\n",
    "    out[\"source_url\"] = out[\"url\"]\n",
    "    out[\"source_domain\"] = out[\"url\"].apply(get_domain)\n",
    "    out[\"title\"] = df.get(\"title\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"claim\"] = \"\"\n",
    "    out[\"label_fine\"] = df.get(\"label\", \"SATIRE\").fillna(\"\").apply(safe_str)\n",
    "    out[\"y\"] = out[\"label_fine\"].apply(map_label_binary)\n",
    "    out[\"text_long\"] = df.get(\"text\", \"\").fillna(\"\").apply(safe_str).apply(normalize_ws)\n",
    "    out[\"text_short\"] = out[\"title\"]\n",
    "    return out\n",
    "\n",
    "dfs = []\n",
    "if PATHS[\"afp\"].exists(): dfs.append(load_afp(PATHS[\"afp\"]))\n",
    "if PATHS[\"factual\"].exists(): dfs.append(load_factual(PATHS[\"factual\"]))\n",
    "if PATHS[\"veridica\"].exists(): dfs.append(load_veridica(PATHS[\"veridica\"]))\n",
    "if PATHS[\"ocr\"].exists(): dfs.append(load_ocr(PATHS[\"ocr\"]))\n",
    "if PATHS[\"tnr\"].exists(): dfs.append(load_tnr(PATHS[\"tnr\"]))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "print(\"Unified rows:\", len(data))\n",
    "display(data[\"dataset\"].value_counts())\n",
    "display(data.head(3))"
   ],
   "id": "29d845752710dadb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified rows: 2125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  dataset                                        id  \\\n",
       "0     NaN  ec5256cca1a28dddab1e6bcea06d7698042da8cb   \n",
       "1     NaN  4010817634d8d58df5de9d932abace7d2a8ec2f5   \n",
       "2     NaN  d11422ae41ff6c23d485c27ccc49b8b1a8e55d88   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://verificat.afp.com/doc.afp.com.32AD84J   \n",
       "1  https://verificat.afp.com/doc.afp.com.32BY897   \n",
       "2  https://verificat.afp.com/doc.afp.com.32CX3EH   \n",
       "\n",
       "                                          source_url source_domain  \\\n",
       "0  https://www.facebook.com/mariana.muntean/posts...  facebook.com   \n",
       "1  https://www.facebook.com/Lupul.Dacic.blog/post...  facebook.com   \n",
       "2  https://www.facebook.com/permalink.php?story_f...  facebook.com   \n",
       "\n",
       "                                               title  \\\n",
       "0  Această înregistrare video nu este o dovadă că...   \n",
       "1  Focarele de variola maimuței nu sunt legate de...   \n",
       "2  Videoclipul care arată o „păpușă Ken însărcina...   \n",
       "\n",
       "                                               claim     label_fine    y  \\\n",
       "0  Acest videoclip arată că aterizarea pe Lună a ...  CONTEXT LIPSĂ  0.0   \n",
       "1  Variola maimuței este provocată de vaccinul As...  CONTEXT LIPSĂ  0.0   \n",
       "2  Aceasta este o nouă păpușă Ken, care este însă...         SATIRĂ  0.0   \n",
       "\n",
       "  text_long                                         text_short  \n",
       "0            Această înregistrare video nu este o dovadă că...  \n",
       "1            Focarele de variola maimuței nu sunt legate de...  \n",
       "2            Videoclipul care arată o „păpușă Ken însărcina...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>title</th>\n",
       "      <th>claim</th>\n",
       "      <th>label_fine</th>\n",
       "      <th>y</th>\n",
       "      <th>text_long</th>\n",
       "      <th>text_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ec5256cca1a28dddab1e6bcea06d7698042da8cb</td>\n",
       "      <td>https://verificat.afp.com/doc.afp.com.32AD84J</td>\n",
       "      <td>https://www.facebook.com/mariana.muntean/posts...</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>Această înregistrare video nu este o dovadă că...</td>\n",
       "      <td>Acest videoclip arată că aterizarea pe Lună a ...</td>\n",
       "      <td>CONTEXT LIPSĂ</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>Această înregistrare video nu este o dovadă că...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4010817634d8d58df5de9d932abace7d2a8ec2f5</td>\n",
       "      <td>https://verificat.afp.com/doc.afp.com.32BY897</td>\n",
       "      <td>https://www.facebook.com/Lupul.Dacic.blog/post...</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>Focarele de variola maimuței nu sunt legate de...</td>\n",
       "      <td>Variola maimuței este provocată de vaccinul As...</td>\n",
       "      <td>CONTEXT LIPSĂ</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>Focarele de variola maimuței nu sunt legate de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>d11422ae41ff6c23d485c27ccc49b8b1a8e55d88</td>\n",
       "      <td>https://verificat.afp.com/doc.afp.com.32CX3EH</td>\n",
       "      <td>https://www.facebook.com/permalink.php?story_f...</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>Videoclipul care arată o „păpușă Ken însărcina...</td>\n",
       "      <td>Aceasta este o nouă păpușă Ken, care este însă...</td>\n",
       "      <td>SATIRĂ</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>Videoclipul care arată o „păpușă Ken însărcina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text inputs for each model",
   "id": "ae63b30713d429e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:19:05.719497Z",
     "start_time": "2025-12-27T16:19:05.259291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data[\"text_long\"] = data[\"text_long\"].fillna(\"\").astype(str)\n",
    "data[\"text_short\"] = data[\"text_short\"].fillna(\"\").astype(str)\n",
    "data[\"title\"] = data[\"title\"].fillna(\"\").astype(str)\n",
    "data[\"claim\"] = data[\"claim\"].fillna(\"\").astype(str)\n",
    "\n",
    "def choose_binary_input(row) -> str:\n",
    "    tl = row[\"text_long\"]\n",
    "    ts = row[\"text_short\"]\n",
    "    if isinstance(tl, str) and len(tl) >= 200:\n",
    "        header = ts.strip()\n",
    "        if header:\n",
    "            return f\"[SHORT] {header}\\n[LONG] {tl}\".strip()\n",
    "        return tl.strip()\n",
    "    return ts.strip()\n",
    "\n",
    "SENT_SPLIT_RE = re.compile(r\"(?<=[\\.!?])\\s+\")\n",
    "\n",
    "def choose_clickbait_input(row, max_chars: int = 250) -> str:\n",
    "    t = row.get(\"title\",\"\").strip()\n",
    "    if t:\n",
    "        return t[:max_chars]\n",
    "    c = row.get(\"claim\",\"\").strip()\n",
    "    if c:\n",
    "        return c[:max_chars]\n",
    "    body = row.get(\"text_long\",\"\").strip()\n",
    "    if body:\n",
    "        first = SENT_SPLIT_RE.split(body)[0].strip()\n",
    "        return first[:max_chars] if first else body[:max_chars]\n",
    "    return \"\"\n",
    "\n",
    "data[\"text_input_veracity\"] = data.apply(choose_binary_input, axis=1).fillna(\"\").astype(str)\n",
    "data[\"text_input_clickbait\"] = data.apply(choose_clickbait_input, axis=1).fillna(\"\").astype(str)\n",
    "data[\"text_len\"] = data[\"text_input_veracity\"].str.len()\n",
    "data[\"has_source_domain\"] = data[\"source_domain\"].fillna(\"\").astype(str).str.len().gt(0).astype(int)\n",
    "\n",
    "data_bin = data[data[\"y\"].isin([0,1])].copy()\n",
    "data_bin = data_bin[data_bin[\"text_len\"] >= 30].copy()\n",
    "\n",
    "data_bin[\"text_hash\"] = data_bin[\"text_input_veracity\"].apply(lambda s: md5(normalize_ws(s).lower()[:2000]))\n",
    "print(\"Binary rows for fusion:\", len(data_bin))\n",
    "display(data_bin[[\"dataset\",\"y\"]].value_counts().head(20))"
   ],
   "id": "90c379174cde8004",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary rows for fusion: 2118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train-val-test splits",
   "id": "7b717adbec1d0ad3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:19:35.203041Z",
     "start_time": "2025-12-27T16:19:28.365186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss1.split(data_bin, groups=data_bin[\"text_hash\"]))\n",
    "train = data_bin.iloc[train_idx].copy()\n",
    "test = data_bin.iloc[test_idx].copy()\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, val_idx = next(gss2.split(train, groups=train[\"text_hash\"]))\n",
    "train2 = train.iloc[tr_idx].copy()\n",
    "val = train.iloc[val_idx].copy()\n",
    "\n",
    "print(\"Train:\", len(train2), \"Val:\", len(val), \"Test:\", len(test))\n",
    "print(\"Train label:\", train2[\"y\"].value_counts().to_dict())\n",
    "print(\"Val   label:\", val[\"y\"].value_counts().to_dict())\n",
    "print(\"Test  label:\", test[\"y\"].value_counts().to_dict())"
   ],
   "id": "66d65a033927a561",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1355 Val: 339 Test: 424\n",
      "Train label: {0.0: 1074, 1.0: 281}\n",
      "Val   label: {0.0: 280, 1.0: 59}\n",
      "Test  label: {0.0: 337, 1.0: 87}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load source_veracity table",
   "id": "46a7349794a2f2df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:19:56.688200Z",
     "start_time": "2025-12-27T16:19:56.643584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_tbl = pd.read_csv(SOURCE_TABLE_PATH, encoding=\"utf-8\")\n",
    "source_tbl[\"source_domain\"] = source_tbl[\"source_domain\"].fillna(\"\").astype(str).str.lower().str.replace(\"www.\",\"\", regex=False)\n",
    "DOMAIN2SCORE = dict(zip(source_tbl[\"source_domain\"], source_tbl[\"source_score_final\"]))\n",
    "\n",
    "def source_score_for_domain(domain: str) -> float:\n",
    "    d = (domain or \"\").strip().lower().replace(\"www.\",\"\")\n",
    "    if not d:\n",
    "        return 0.0\n",
    "    if d in PLATFORM_DOMAINS:\n",
    "        return 0.0\n",
    "    return float(DOMAIN2SCORE.get(d, 0.0))\n",
    "\n",
    "for df_ in (train2, val, test):\n",
    "    df_[\"source_score\"] = df_[\"source_domain\"].apply(source_score_for_domain)\n",
    "    df_[\"p_true_source\"] = df_[\"source_score\"].apply(lambda z: 1/(1+np.exp(-z)))\n",
    "\n",
    "display(train2[[\"source_domain\",\"source_score\",\"p_true_source\"]].head(5))"
   ],
   "id": "b04b9337846ccf7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  source_domain  source_score  p_true_source\n",
       "0  facebook.com           0.0            0.5\n",
       "1  facebook.com           0.0            0.5\n",
       "2  facebook.com           0.0            0.5\n",
       "4  facebook.com           0.0            0.5\n",
       "5  facebook.com           0.0            0.5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_domain</th>\n",
       "      <th>source_score</th>\n",
       "      <th>p_true_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load transformer models",
   "id": "79d351150d5cde63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:31:00.486537Z",
     "start_time": "2025-12-27T16:29:20.288788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def load_hf_binary_model(model_dir: Path):\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    mdl.to(device)\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "ver_tok, ver_mdl = load_hf_binary_model(VERACITY_MODEL_DIR)\n",
    "cb_tok, cb_mdl = load_hf_binary_model(CLICKBAIT_MODEL_DIR)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(tok, mdl, texts, batch_size: int = 16, max_length: int = 512):\n",
    "    probs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\", leave=False):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tok(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        out = mdl(**enc)\n",
    "        logits = out.logits\n",
    "        if logits.shape[-1] == 1:\n",
    "            p1 = torch.sigmoid(logits).squeeze(-1)\n",
    "        else:\n",
    "            p1 = torch.softmax(logits, dim=-1)[:, 1]\n",
    "        probs.extend(p1.detach().cpu().numpy().tolist())\n",
    "    return np.array(probs, dtype=float)\n",
    "\n",
    "train2[\"p_true_content\"] = predict_proba(ver_tok, ver_mdl, train2[\"text_input_veracity\"].tolist(), batch_size=16, max_length=512)\n",
    "val[\"p_true_content\"]    = predict_proba(ver_tok, ver_mdl, val[\"text_input_veracity\"].tolist(), batch_size=16, max_length=512)\n",
    "test[\"p_true_content\"]   = predict_proba(ver_tok, ver_mdl, test[\"text_input_veracity\"].tolist(), batch_size=16, max_length=512)\n",
    "\n",
    "train2[\"p_clickbait\"] = predict_proba(cb_tok, cb_mdl, train2[\"text_input_clickbait\"].tolist(), batch_size=32, max_length=128)\n",
    "val[\"p_clickbait\"]    = predict_proba(cb_tok, cb_mdl, val[\"text_input_clickbait\"].tolist(), batch_size=32, max_length=128)\n",
    "test[\"p_clickbait\"]   = predict_proba(cb_tok, cb_mdl, test[\"text_input_clickbait\"].tolist(), batch_size=32, max_length=128)\n",
    "\n",
    "display(train2[[\"p_true_content\",\"p_clickbait\",\"source_score\",\"y\"]].head(5))"
   ],
   "id": "e75248859b3a27c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/85 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b14e0a4131f47248018f594a2628473"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/22 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4e221d2106f40b980ed6edec5235425"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/27 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f7602f7d1474736a2c6eb4590b55d2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/43 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21e85c3862e64597a5d21f2da12f493d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75e20776349c4e8798447e8673f14f3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/14 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf41605f2674446786f109a7a3050eb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   p_true_content  p_clickbait  source_score    y\n",
       "0        0.000636     0.155183           0.0  0.0\n",
       "1        0.001183     0.091858           0.0  0.0\n",
       "2        0.000743     0.142920           0.0  0.0\n",
       "4        0.000670     0.135450           0.0  0.0\n",
       "5        0.000734     0.136982           0.0  0.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_true_content</th>\n",
       "      <th>p_clickbait</th>\n",
       "      <th>source_score</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.155183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.142920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.135450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.136982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Logistic regression + Grid Search",
   "id": "498e773e9267d6e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:32:25.598905Z",
     "start_time": "2025-12-27T16:32:20.679815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def make_fusion_X(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = pd.DataFrame()\n",
    "    X[\"logit_p_true_content\"] = df[\"p_true_content\"].apply(lambda p: logit(float(p)))\n",
    "    X[\"logit_p_not_clickbait\"] = df[\"p_clickbait\"].apply(lambda p: logit(1.0 - float(p)))\n",
    "    X[\"source_score\"] = df[\"source_score\"].astype(float)\n",
    "    X[\"text_len\"] = df[\"text_len\"].astype(float)\n",
    "    X[\"has_source\"] = df[\"has_source_domain\"].astype(float)\n",
    "    return X\n",
    "\n",
    "X_tr = make_fusion_X(train2)\n",
    "y_tr = train2[\"y\"].astype(int).values\n",
    "X_va = make_fusion_X(val)\n",
    "y_va = val[\"y\"].astype(int).values\n",
    "X_te = make_fusion_X(test)\n",
    "y_te = test[\"y\"].astype(int).values\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=5000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"lbfgs\",\n",
    "    ))\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid={\"clf__C\":[0.1,0.3,1.0,3.0,10.0]},\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "grid.fit(X_tr, y_tr)\n",
    "fusion_model = grid.best_estimator_\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV f1_macro:\", grid.best_score_)\n",
    "\n",
    "p_va = fusion_model.predict_proba(X_va)[:,1]\n",
    "pred_va = (p_va >= 0.5).astype(int)\n",
    "print(\"VAL @0.5\")\n",
    "print(classification_report(y_va, pred_va, target_names=[\"FALSE\",\"TRUE\"]))\n",
    "\n",
    "p_te = fusion_model.predict_proba(X_te)[:,1]\n",
    "pred_te = (p_te >= 0.5).astype(int)\n",
    "print(\"TEST @0.5\")\n",
    "print(classification_report(y_te, pred_te, target_names=[\"FALSE\",\"TRUE\"]))"
   ],
   "id": "70e40f1d1bcd0045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best params: {'clf__C': 0.1}\n",
      "Best CV f1_macro: 0.9796593534486184\n",
      "VAL @0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE       1.00      1.00      1.00       280\n",
      "        TRUE       0.98      0.98      0.98        59\n",
      "\n",
      "    accuracy                           0.99       339\n",
      "   macro avg       0.99      0.99      0.99       339\n",
      "weighted avg       0.99      0.99      0.99       339\n",
      "\n",
      "TEST @0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE       1.00      0.99      0.99       337\n",
      "        TRUE       0.95      0.99      0.97        87\n",
      "\n",
      "    accuracy                           0.99       424\n",
      "   macro avg       0.97      0.99      0.98       424\n",
      "weighted avg       0.99      0.99      0.99       424\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pick decision threshold (based on F1)",
   "id": "53195d7bc3d6e3a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:34:20.576991Z",
     "start_time": "2025-12-27T16:34:20.236161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def best_threshold(y_true, p_true):\n",
    "    best_t, best_s = 0.5, -1\n",
    "    for t in np.linspace(0.05, 0.95, 181):\n",
    "        y_pred = (p_true >= t).astype(int)\n",
    "        s = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        if s > best_s:\n",
    "            best_s, best_t = s, float(t)\n",
    "    return best_t, best_s\n",
    "\n",
    "t_star, s_star = best_threshold(y_va, p_va)\n",
    "print(\"Best threshold (val, f1_macro):\", t_star, \"score:\", s_star)\n",
    "\n",
    "pred_te_star = (p_te >= t_star).astype(int)\n",
    "print(\"TEST @t*\")\n",
    "print(classification_report(y_te, pred_te_star, target_names=[\"FALSE\",\"TRUE\"]))"
   ],
   "id": "e507d2631b5e7aaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold (val, f1_macro): 0.3549999999999999 score: 0.9897397094430993\n",
      "TEST @t*\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE       1.00      0.98      0.99       337\n",
      "        TRUE       0.93      0.99      0.96        87\n",
      "\n",
      "    accuracy                           0.98       424\n",
      "   macro avg       0.97      0.99      0.98       424\n",
      "weighted avg       0.98      0.98      0.98       424\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:35:34.931262Z",
     "start_time": "2025-12-27T16:35:34.918192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib, datetime, json\n",
    "\n",
    "model_path = OUT_FUSION_DIR / \"fusion_lr.joblib\"\n",
    "joblib.dump(fusion_model, model_path)\n",
    "\n",
    "threshold_payload = {\n",
    "    \"threshold\": t_star,\n",
    "    \"selected_on\": \"val\",\n",
    "    \"metric\": \"f1_macro\",\n",
    "    \"val_score\": s_star,\n",
    "    \"created_at_utc\": datetime.datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "    \"features\": [\n",
    "        \"logit_p_true_content\",\n",
    "        \"logit_p_not_clickbait\",\n",
    "        \"source_score\",\n",
    "        \"text_len\",\n",
    "        \"has_source\",\n",
    "    ],\n",
    "}\n",
    "(OUT_FUSION_DIR / \"fusion_threshold.json\").write_text(\n",
    "    json.dumps(threshold_payload, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "schema_payload = {\n",
    "    \"feature_order\": threshold_payload[\"features\"],\n",
    "    \"notes\": \"Features are scaled with StandardScaler. source_score is log-odds from the source prior table.\",\n",
    "}\n",
    "(OUT_FUSION_DIR / \"fusion_feature_schema.json\").write_text(\n",
    "    json.dumps(schema_payload, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"Saved:\", model_path.resolve())\n",
    "print(\"Saved:\", (OUT_FUSION_DIR / \"fusion_threshold.json\").resolve())\n",
    "print(\"Saved:\", (OUT_FUSION_DIR / \"fusion_feature_schema.json\").resolve())"
   ],
   "id": "99d83ca3c305c425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Programming\\AI\\AI-Self\\NLP-FakeNews-Detection-Classifier\\fusion\\artifacts\\fusion\\fusion_lr.joblib\n",
      "Saved: D:\\Programming\\AI\\AI-Self\\NLP-FakeNews-Detection-Classifier\\fusion\\artifacts\\fusion\\fusion_threshold.json\n",
      "Saved: D:\\Programming\\AI\\AI-Self\\NLP-FakeNews-Detection-Classifier\\fusion\\artifacts\\fusion\\fusion_feature_schema.json\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper",
   "id": "7ea6e219889370a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:35:55.566917Z",
     "start_time": "2025-12-27T16:35:55.538912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fusion_predict_one(p_true_content: float, p_clickbait: float, source_score: float, text_len: int, has_source: int):\n",
    "    X = pd.DataFrame([{\n",
    "        \"logit_p_true_content\": logit(float(p_true_content)),\n",
    "        \"logit_p_not_clickbait\": logit(1.0 - float(p_clickbait)),\n",
    "        \"source_score\": float(source_score),\n",
    "        \"text_len\": float(text_len),\n",
    "        \"has_source\": float(has_source),\n",
    "    }])\n",
    "    return float(fusion_model.predict_proba(X)[:,1][0])\n",
    "\n",
    "row = train2.iloc[0]\n",
    "p = fusion_predict_one(\n",
    "    row[\"p_true_content\"], row[\"p_clickbait\"], row[\"source_score\"],\n",
    "    int(row[\"text_len\"]), int(row[\"has_source_domain\"])\n",
    ")\n",
    "print(\"Example final_p_true:\", p, \"y:\", int(row[\"y\"]))"
   ],
   "id": "147898d68d22bc40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example final_p_true: 0.012740228644084068 y: 0\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e9ab826f60471ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
